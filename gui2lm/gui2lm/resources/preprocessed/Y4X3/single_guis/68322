Even though it is still partly able to capture the structure, it generally struggles to predict elements that which number is higher than 4 (see Table \ref{tabelle1}). 

% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{subfig}
\usepackage{listings}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{array}
\usepackage{amssymb}
\newcolumntype{?}{!{\vrule width 1pt}}

\lstset{
     escapeinside={(*}{*)}         % if you want to add LaTeX within your code
}
% \bibliography{refs}
% \addbibresource{refs.bib}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage{float}
% \restylefloat{table}


\begin{document}
%
\title{Using Language Modelling for Generative GUI Prototyping}
%
\titlerunning{Using Language Modelling for Generative GUI Prototyping}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Davis Siemens\\Matriculation Nr.: 1645656}

%
\authorrunning{D. Siemens}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Institute for Enterprise Systems, Mannheim, Germany\\
\email{dsiemens@mail.uni-mannheim.de}
}
%
\maketitle              % typeset the header of the contribution
%

\begin{abstract}
GUIs are omnipresent in all modern applications. They are the primary, if not the only, interface between the application and the user. Therefore, an application is only as good as its GUI, and if the GUI is not well designed, so is the application. 
\\
Designing a good GUI is very challenging. Not only must design rules and principles be followed to ensure intuitive use. It also requires creativity and innovation while meeting customer requirements. Therefore professional GUI designers are needed. To support them in their work and to save resources, we contribute to a system that can automatically generate GUI designs. By describing GUIs as a sequence of components, we can create a language model that creates GUI layouts given such an initial sequence. GUI designers could use this tool to create a number of low-fidelity mock-ups of alternative GUI layouts. These mock-ups can then be used as a basis for further work or directly be used in further interviews with the end customer.
Within this work, we contribute to the development of such a system by creating a method to abstract GUIs so a state-of-the-art language model can process them. The named method, therefore, unlocks the potential of big GUI data sets and deep learning language generation frameworks to generate User Interfaces automatically.


% "People don't know what they want until you show it to them" - Steve Jobs 
% \\
% In any system development process is the most crucial step arguably requirement elicitation. A step, that is especially challenging if the customers are not sure about what they want themselves, making it difficult to grasp requirements. Talking about the creation of  \textit{Graphical User Interfaces} (GUIs), a tool that automatically drafts GUIs based on natural language (NL) could overcome this barrier. 
% The name approach could be used in an interactive elicitation session with the customer to support the requirement engineering process. The customer describes his requirements in natural language, the analyst uses these language fragments and feeds them into the approach to directly get suggestions for GUI mock-ups (e.g. in which the system generates several variants). These GUIs can then be used directly in further elicitation to visualize, reduce misunderstandings, and else. 
% Within this work, we contribute to the development of such a system by creating a method to abstract GUIs into data that can be processed by NL translation models. The named method therefore unlocks the potential of big GUI data sets and deep learning technology to automatically generate User Interfaces.  


\keywords{Language Model \and Natural Language Generation \and GUI-Design}
\end{abstract}
%
%
%
\section{Introduction}\label{INTRO}
User Interfaces (UIs) are the bridges between humans and computers. They are the main point of interactions and enable us to give commands to the computer, read results, and way more. The most common and important type of UI is the graphical UI (GUI). In most modern applications, they are either the only or primary interface for the user. Having such importance leads to high expectations for an application's GUI. There are only a few other areas with so many design rules and best practices to ensure good usability and acceptance. Also, it is shown that if a GUI is poorly designed and does not support the application's functionalities, it results in less usage and loyalty of users in a negative way.

Furthermore, the GUI must not only be well designed in a practical sense. Aesthetics plays an important role as well. Customers, and all other stakeholders as well, have a specific vision about how the GUI must function and how it should look. Both are sometimes even incompatible with each other. 

Designing an easy-to-use and intuitive GUI is, therefore, not an easy task. A prerequisite for a successful GUI design process is thoroughly understanding and anticipating the stakeholders' expectations. 
To capture these expectations, a requirement engineering process is indispensable. Therefore, requirement engineering always starts with a requirements elicitation process, where the requirements for a system are explored and elicited from users, customers, and other stakeholders\cite{Elastitation8138263}.

Once requirements are understood, the GUI Designer must implement them while following best practices in the form of design guidelines and principles\cite{GalitzGUI}\cite{MCKAYGUI}. 

To overcome these challenges, GUI designers often reuse existing GUIs and modify them to their needs, leading to the development of multiple GUI retrieval models and search engines based on query search e.g. \cite{KristianWork} or \cite{GUIRetrievalIntro}. 

While this approach works sufficiently it has some restrictions, as already analysed in \cite{guigan}. First, the designer must structure his initial visual design idea in a natural language query, which can lead to representation gaps\cite{guigan}. Further, repeatedly reusing old and potentially outdated GUI designs will result in less creative work and more alike GUIs, eventually leading to a staling innovation\cite{guigan}.

We propose an alternative GUI creation approach. Within this work, we will contribute to a system that can automatically generate GUI layouts using a language generation framework. GUI-Designers can use this system to extend initial design ideas and generate alternative low-fidelity GUI mock-ups. Based on the mock-ups, a completely new GUI design can be developed in a more efficient and innovative way. Also, the automatically created GUIs can be used to clarify and visualize the customer expectations and reduce misunderstandings on the spot during elicitation. 

This thesis will take the first steps towards the development of the aforementioned system. 

%---------------------------
%GUIs are omnipresent in all modern applications. They are the primary, if not the only, interface between the application and the user. Therefore, an application is only as good as its GUI, and if the GUI is well designed, the application will be easy-to-use. If the GUI is not intuitive enough or does not support the operation of the application optimally, then this negatively affects the loyalty of the end-user\cite{Jansen1998TheGU}.
%However, designing an easy-to-use and intuitive GUI is not an easy task. It is a complex and time-consuming process, starting with understanding the customer's expectations. It is important that each GUI matches the customer's or other stakeholder's visions. To capture these, a requirement engineering process is indispensable. Requirement engineering always starts with a requirements elicitation process, where the requirements for a system are explored and elicited from users, customers, and other stakeholders\cite{Elastitation8138263}.Once requirements are understood, the GUI-Designer has to implement them while following best practices in the form of design guidelines and principles\cite{GalitzGUI}\cite{MCKAYGUI}. Since the GUI is a visual bridge between user and application it has to meet aesthetic standards and current trends. To overcome these challenges, GUI designers are often reusing existing GUIs and modifying them to their needs. This led to the development of multiple GUI retrieval models and search engines based on query search e.g. \cite{KristianWork} or \cite{GUIRetrievalIntro}. While this approach works sufficiently it has some restrictions, as already analysed in \cite{guigan}. First, the designer must structure his initial visual design idea in a natural language query, which can lead to representation gaps. Further, reusing old and potentially outdated GUI designs will hinder innovation.
%which requires long experience in specific domains. One must follow particular GUI design guidelines and principles\cite{GalitzGUI} \cite{MCKAYGUI}.
% However, designing an easy-to-use and intuitive GUI is not an easy task. It is a complex and time-consuming process, starting with understanding the customer's expectations. It is important that each GUI matches the customer's or other stakeholder's visions. To capture these, a requirement engineering process is indispensable. Requirement engineering always starts with a requirements elicitation process, where the requirements for a system are explored and elicited from users, customers, and other stakeholders\cite{Elastitation8138263}.
% To ease this process GUI mock-ups are often designed early in the process to be presented to the subjects. They function as a way to communicate and materialize functional and non-functional requirements. On the basis of mock-ups, the customer can align their vision of the system with the requirements analyst understanding. The critical problem here is that eliciting requirements and prototyping Mock-Ups is an interactive process that is very resource-intensive. 
% \\
% To enable an economic way to elicitate requirements, automation is needed. A tool could do so by automatically drafting GUI mock-ups based on requirements formulated by the user or customer in NL.
% With such a system the analyst could use these language queries to directly get suggestions for GUI mock-ups in an instant (e.g. in which the system generates several variants). These GUIs can then be used to clarify and visualize the customer expectations and reduce misunderstandings on the spot. 
% \\
% The first steps towards the development of the aforementioned system will be taken in this bachelor thesis. 


\subsection{Goal of this thesis}
In this work, we present a method for abstracting GUIs so that they can be processed by a modern language model. The aforementioned method therefore makes it possible to train deep learning applications on large GUI datasets and enabling automatic GUI generation.
\\
The focus of this thesis is:
\begin{itemize}
\item The creation of an abstract representation of GUIs, which is suited to be used in deep learning language model
\item The evaluation of the GUI abstractions in regards to how well it is suited to be utilized by deep learning systems 
\end{itemize}
This work will also cover: 
\begin{itemize}
\item The design of a state-of-the-art deep learning model, able to generate GUI representations
\item The implementation of the deep-learning application
\end{itemize}
The thesis will answer the following question: 
\begin{itemize}
\item[R1] How can GUIs be abstracted into a representation to train language models to generate novel GUI designs?
\item[R2] How effective, measured by it's perplexity, will the language model be in generating GUIs?
\end{itemize}
The thesis is structured approximately as follows: 


Section 2 will list related research and explain correlations and differences to this work. 
Section 3 will explain the technologies and models used to give background and context.
Section 4 will collaborate on the used approach to abstract GUIs for language modeling. 
Section 5 will document the technical implementation of the deep learning application.
Section 6 will test the resulting implementation and evaluate it the GUI generation abilities. 
Section 7 Discussed the evaluation outcome and concludes lessons learned. 
In Section 8 acknowledgments are stated, and in Section 9 further information is given.

\section{Related Work}
Even though the art of automatic GUI generation is a novel topic, related work has been conducted in the past. This chapter provides an overview of previous related work and shows how this thesis can be embedded in earlier work. I will also distinguish our work from previous research and explain why we call our approach novel.
\newline
\newline
There has been a long history of neural networks being utilized to automatically generated creative works like images or paintings. Oftentimes, this is done based on a users instructions, describing the desired outcome. For example, the \textit{Art Maker}\footnote{\url{https://hotpot.ai/art-maker?s=site-menu}} of \textit{Hotpot AI}, an online tool that creates creative paintings based on short sentences. It is an AI trained on a large dataset of paintings linked to text descriptions. Based on this data, the tool is able to process the needs of users and produce "creative" works based on them. 

Looking deeper into that topic, one can see that there have been various research to create machine learning tools that generate layouts to support users in creative tasks like UI design, graphic design, etc.

LayoutGAN\cite{layoutgan} is a framework that uses a novel Generative Adversital Network (GAN) approach that generates realistic layouts by modeling geometric relations of different types of graphic 2D elements \cite{layoutgan}. A GAN is a neural network framework well suited for generating natural-looking images, but which also has not yet been utilized well for creating designs\cite{layoutgan}. With the novel proposed GAN called LayoutGAN, they want to overcome traditional weak points resulting from GANs synthesizing layouts in a pixel space\cite{layoutgan}. 
LayoutGAN consists of two components. First is the generator, which generates realistic 2D layouts based on graphic elements randomly placed in a 2D landscape\cite{layoutgan}. It does so by using self-attention to filter and process their labels and geometric attributes\cite{layoutgan}. 
The second is the wireframe rendering layer, which renders the priorly generated layout to a wireframe\cite{layoutgan}. By using a machine learning classifier, it optimizes the layouts and generates an accurate alignment\cite{layoutgan}.


Like LayoutGan, LayoutTransformer\cite{layouttransformer} is a machine learning framework that learns on relationships of graphic elements in a layout and then creates new layouts in domains including mobile applications, 3D objects, graphic design, ...\cite{layouttransformer}. It does so by leveraging self-attention to analyze given graphical primitives and modeling them as composal attributes\cite{layouttransformer}. It then generates new layouts either freely or based on an initial set of primitives, the so-called seed\cite{layouttransformer}. A prerequisite for the framework is that the processed layouts are decomposed in layout primitives, which is a limitation but also allows it to be utilized in a range of different domains\cite{layouttransformer}. In Figure \ref{LayoutGanExample} example generations from LayoutGan are shown. 
\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{LayoutGanExample.png}
  \caption{This image from \cite{layouttransformer} shows example generations of 3d objects from the LayoutTransformer. The first row shows the input to the models\cite{layouttransformer}. They are the input primitives. The second row shows the generated layouts based on the seeds\cite{layouttransformer}.}
  \label{LayoutGanExample}
\end{figure}


Arroyo et al.\cite{VTNarroyo2021variational} create a framework with the goal to create new novel layouts in different domains like documents, user interfaces, ... \cite{VTNarroyo2021variational}. 
It does so again by utilizing self-attention layers to capture relationships between graphic elements\cite{VTNarroyo2021variational}. These relationships then get processed by a Variational Autoencoder (VAE), a neural network infrastructure known for performing well in image translation, -synthesis, and text generation\cite{VTNarroyo2021variational}. The paper then proposes a Variational Transformer Network (VTN), which can be seen as an instance of a VAE made for layout generation\cite{VTNarroyo2021variational}. The VTN can capture design rules based on the training data and create new layout samples\cite{VTNarroyo2021variational}. It then generates new layouts based on an initial input vector and those design rules it generates new layouts. Besides layout generation, the VTN could also be utilized to detect layout in existing images\cite{VTNarroyo2021variational}. 

The tools named so far are similar to ours since they create novel, realistic layouts using machine learning. The difference is that we solely focus on the domain of mobile app GUI designs. Also, we utilize different machine learning frameworks that create layouts similar to \textit{Natural Language Generation}, which will be explained further in Section \ref{Background} and \ref{Approach}. The key difference, however, is the novelty of our approach to preprocessing our data, making it usable for Natural Language Generation. Furthermore, focusing on one domain allows us to emphasize how to abstract our data more than on the actual neural network framework processing this data. 
\subsubsection{More closely related works}
Next to our work, more tools leverage machine learning technology to are providing their users with creative mobile app GUI layouts. All of these are using the same dataset RICO \cite{RICOStandard}, which was published in 2017 and set a milestone in this research topic, also enabling our work. More background and content of the dataset will be given in Section \ref{Approach}. 
\newline
\newline
GUI2WiRe\cite{KristianWork} is a model already managing GUI prototype retrieval from the mobile application GUI database RICO by simple keyword-based searches, utilizing natural language queries\cite{KristianWork}. A user can specify his desired GUI by describing it in a short text sequence\cite{KristianWork}. The tool then utilizes different Information Retrieval models and Automatic Query Expansion technologies to retrieve the best fitting GUI prototype from the database\cite{KristianWork}. The required information is gathered from text sequences retrieved from the GUI data in the RICO dataset\cite{KristianWork}. GUI2WiRe also offers an embedded graphical editor, which can be used to modify the retrieved prototypes on the spot\cite{KristianWork}. 


GUI2WiRe does not generate novel GUI prototypes; it rather fetches already existing GUIs from a database. Even though this approach can help users with initial design ideas and rapid prototyping, it creates the problem of always reusing existing designs, eventually leading to a stale innovation. Also, the tool works by using short text descriptions of the desired GUI design from its user. Even though our tool will work similarly to natural language generation, it will not process natural language but rather treat GUIs like natural language. Therefore, we will not generate GUIs based on text sequence seeds but based on the initial elements of the GUIs themselves. 
\newline
\newline
A novel study by Huang et al. (2021) is designing three different approaches based on deep-learning to provide simple mobile app GUI designs from queries in natural language\cite{GoogleHuang2021creating}.
The first two approaches are, like GUI2WiRe, retriever models which fetch existing GUIs from a database\cite{GoogleHuang2021creating}. However, the third approach is more closely related to our work by being generative\cite{GoogleHuang2021creating}.


The first approach is called \textit{Text-only Retriever}\cite{GoogleHuang2021creating}. It is a retriever framework being trained on a dataset linking high-level text descriptions to specific UIs of the RICO dataset\cite{GoogleHuang2021creating}. It then retrieves UIs by the similarity of the search query to these high-level descriptions\cite{GoogleHuang2021creating}. 
The second approach is called \textit{Multi-Modal Retriever} and goes one step further\cite{GoogleHuang2021creating}. It does not only utilize the high-level text descriptions of the GUIs but also considers details of the UI. By putting them into correlation, it retrieves GUIs based on text queries\cite{GoogleHuang2021creating}. 


The third approach called \textit{UI-Generator}, does not retrieve but creates new GUIs based on the provided natural language query\cite{GoogleHuang2021creating}. It handles this task like a sequence-to-sequence translation problem by translating the text query to a sequence of GUI elements\cite{GoogleHuang2021creating}. To do so, Huang et al. (2021) implement a transformer Encoder-Decoder Architecture\cite{GoogleHuang2021creating}. 
In such an Encoder-Decoder structure, the encoder maps a variable-length sequence to a fixed-length code\cite{AttentionIsAllYouNeed}. The decoder then generates an output sequence of symbols by decoding the given code and therefore translating the input sequence\cite{AttentionIsAllYouNeed}.
In this case the encoder is given a text description in form of a natural language query \textit{t(1),...,t(n)} which it encodes to a sequence\cite{GoogleHuang2021creating}. The Decoder is then a model that is trained on a GUI dataset. It uses the encodings and autoregressively outputs a sequence of GUI elements \textit{g(1),…,g(m)}\cite{GoogleHuang2021creating}. It does so by creating one GUI element at a time \textit{g(i)} while considering all previous created GUI elements \textit{g(i),…,g(i-1)}, always starting with \textit{g(1)} as the starting token\cite{GoogleHuang2021creating}. 

\begin{figure}[h!]
\centering
   \includegraphics[width=0.8\textwidth]{Google_Generator_.pdf}
  \caption{This image from \cite{GoogleHuang2021creating} shows example generations of the  from the \textit{UI-Generator}. The left shows the text query while the images show the GUI prototypes.}
  \label{LayoutGanExample}
\end{figure}

If compared to Huang et al. (2021), our tool can be seen as the decoder architecture in an encoder-decoder structure. 
However, differing from Huang et al. (2021), we generate GUI Designs in a way that is similar to \textit{Natural Language Generation} by treating the GUIs like a sequence of its elements. We do not translate a text description to a GUI Design. 
Both works also differ in how there model represents a GUI as a sequence, with Huan et al. (2021) sorting them after their approximate Y and X coordinates\cite{GoogleHuang2021creating}.
\newline
\newline
GUIGAN\cite{guigan} is a closely related work, proposing a tool that splits mobile app GUIs into single components to reuse them in novel layouts\cite{guigan}.
It does so by splitting existing GUI designs into a subtree of its components, creating a subtree repository\cite{guigan}. The model then serializes those sequences to train a model to create new novel GUI designs using already existing GUI components\cite{guigan}.

GUIGAN is based on SeqGAN\cite{SeqGan}, which can be seen as a GAN designed to generate a sequence of tokens making it usable for \textit{Natural Language Generation}\cite{guigan}. The SeqGAN, is then trained on serialized GUIs from the RICO dataset. During training, the generator creates sequences, which are then compared to the original GUIs from the dataset. After training, GUINAN reuses components of the GUIs in the dataset and combines them in a novel way. 
It is closely related to our work since both models create GUIs in a way that is similar to \textit{Natural Language Generation}\cite{guigan}.
First, the difference between GUIGAN and the proposed work is that we focus on developing a GUI abstraction method rather than designing a UI generation model. Second, our model will generate layouts from new GUI components instead of reusing existing elements from former GUIs. Also, we do not partition the GUIs into a subtree structure but abstractly them into a grid-like structure. 

\begin{figure}[h!]
\centering
   \includegraphics[width=0.8\textwidth]{GUIGAN.pdf}
  \caption{This slightly modified image from \cite{guigan} demonstrates the way GUIGAN generates new GUI designs. GUIGAN works by reusing elements from existing GUIs in a novel way.}
  \label{LayoutGanExample}
\end{figure}

%\subsection{Basis for Further Work}
%Our proposed tool can be used as a model that can help create new GUI design ideas like GuiGAN \cite{guigan}. If given an initial GUI layout idea it can create novel GUI designs, helping to extend ideas as stated in the Section \ref{INTRO}. 
%Further, it can also function as a decoder in an encoder-decoder architecture. Therefore, future work could develop an complementary encoder and transistor to generate GUI designs based on text descriptions, as shown by Huang et al. (2021)\cite{GoogleHuang2021creating}.
\section{Background \& Context}\label{Background}
This chapter provides helpful background knowledge to better understand the approach, implementation, and evaluation of the work conducted in this thesis.
Our goal is to create a machine learning system capable of generating GUI designs in a way that is similar to \textit{Natural Language Generation} (NLG).
\subsection{Natural Language Generation}
In \cite{naturalLanguageGeneration} NLG is defined as: 
\begin{quote}
"... the subfield of artificial intelligence and computational linguistics that is concerned with the construction of computer systems that can produce understandable texts in English or other human languages from some underlying non-linguistic representation of information." (p.57)
\end{quote}
In other words, NLG means that a computer system, e.g., a trained artificial intelligence,  is feeding on underlying information, e.g., datasets, creating text in a natural language, e.g., spoken or written Englisch. 
Nowadays, NLG is used in multiple domains, including chatbots, image labeling, and even weather forecasting. 

An exemplary usage is the tool FOG\cite{FOG}, which produces weather forecast announcements that are readable by humans and usable in the News, Broadcasts, etc. In \cite{FOG} an exemplary query of FOG is given: 
\begin{quote}
"WINDS NORTHWEST 15 DIMINISHING TO LIGHT MONDAY AFTERNOON. CLOUDY WITH OCCASIONAL LIGHT SNOW. FOG PATCHES. VISIBILITIES 2 TO 5 NM IN SNOW" (p. 46)
\end{quote}
%(Goldberg et al., 1994, p. 46)
FOG gathers its information by feeding on a data set containing information about the weather forecast in a specific region. The tool can transform forecast information into natural language by being priorly trained on a data set that links forecast texts to weather data.
This task, simple at first sight, hides many challenges. 
The difficulty of NLG, and its super-category Natural Language Processing (NLP), lies in the nature of human language. Opposite to programming or other formal languages, natural language was not designed but developed over time and variously influenced, leading to its ambiguity\cite{ambiguity}. Humans can deal with this ambiguity by regarding context, history, and other information available; Computers can't. Various NLG techniques have been developed to overcome this challenge, including statistical models based on machine learning. 

Our vision is to leverage those existing NLG concepts and technologies and reuse them not to generate natural language, but GUI Designs. 

\subsection{Language Modelling} \label{LM}
Language Models (LM) are at the core of modern NLG and are generalized as models which assign probabilities to sequences of words\cite{LMDefinition}. Given any sentence \textit{x(1),…,x(n)} with an arbitrary number n of words \(x(i)\in X\), with \(n,i\in \mathbb{N}\), and X being the vocabulary, the LM can assign a probability to the sentence shown in equation \ref{calc1}.
\begin{equation}\label{calc1}
    P(x(1),…,x(n))
\end{equation}

The traditional approach to creating the probability distribution of an LM is to train the model on a large corpus of text data. It is given text sequences e.g. \textit{x(1),…,x(n-1)} from the corpus and then predicts the next word in that sequence e.g. \textit{x´(n)}. By then comparing it to the actual text sequence \textit{x(1),…,x(n)} a probability distribution can be built. 

Such an exemplary corpus of text data could be Shakespeare’s play \textit{Hamlet}. Assuming a suited LM is trained on this specific text corpus, then we could let the model predict the probabilities of the following two sentences: 
\begin{quote}
    $s_1$ = "To be or not to be",
    $s_2$ = "O Romeo, Romeo"
\end{quote}
Doing so, it will predict $P(s_1)>P(s_2)$, since the sequence $s_1$ is more likely to appear in the corpus of \textit{Hamlet}. 
Vice versa, the LM will predict $P(s_1)<P(s_2)$ if trained on \textit{Romeo and Juliet}, another play by Shakespeare. Alternatively, we could feed the model another sentence:
\begin{quote}
    $s_3$ = "Be to not to not or"
\end{quote}
If done so, $s_3$ will have the lowest probability out of all three text sequences since it is not a correct sentence in the English language. 
\subsubsection{Text generation with Language Modelling}
Given a sentence as described above \textit{x(1),…,x(n)}, a LM can calculate it's probability as shown in Equation \ref{calc1}.
However, in a natural language, probabilities are often times conditional. The likelihood of a word appearing in a sentence is dependent upon the prior context of the word. After a sentence like $s_1$ the word \textit{"that"} is more likely to occur than the word \textit{"Romeo"}, since it gives the sequence a sense.  
\[P("Romeo"|s_1)<P("that"|s_1)\]
This characteristic enables the generation of natural language via an LM. If given an initial text sequence$x(1),…,x(n)$, the LM can output a conditional probability distribution $P$ over the Vocabulary $X$. If $P$ is given, the next word $\^x$ for that sequence can be selected. Therefore from all the possible words in the Vocabulary, one word has to be selected to be the next word  $\^x$ in the sequence. 

That method of deciding which word to choose is called sampling technique. 
If we choose our method as always to select the word with the highest conditional probability, it will look like this:
\[Max\ \^x\in X:P(\^x|x(1),…,x(n))\] 
Going back to the example of $s_2$ $\^x$ could be $"that"$. By feeding the LM with its output $\^x$ and adding it to the sentence, the LM can generate text sequences iteratively. 
To now generate entire sentences from an LM,  the $x(1),…,x(n),\^x$ and therefore \textit{"to be or not to be that"} can be fed to the LM to predict the next word. Done iteratively, the model could eventually predict \textit{" To be or not to be that is the question"}. The process is visualized in Figure \ref{NLGviaLM}

\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{LM_NLG_1.drawio-3.eps}
  \caption{A visualisation of how NLG functions with a LM. First, the model predicts an probability distribution over a vocabulary. Then the next words gets sampled and used as a input for the following steps.}
  \label{fig8}
  \label{NLGviaLM}
\end{figure}

\subsection{Modern Approach to Language Modelling}\label{Modern Approach to LM}
Language modeling is widely done with neural networks\cite{lstm_basics_one}. The current state of the art for such a task are \textit{Long Short-Term Memory} (LSTM) architectures. They are a type of \textit{Recurrent Neural Networks} (RNN). RNNs are generally designed to work with sequential data, where single data points are dependent on previous data points in the sequence, a characteristic essential for our use case. For this reason, when predicting a word in a sequence, RRNs take into account all of the predecessor words to create a probability\cite{lstm_basics_one}. 


However, RNNs are well known to be hard to train on long sequences due to the vanishing gradient problem\cite{vanishing_gradient_problem}. Gradients are utilized to update weights within a neural network. While training an RNN on a long sequence of data points, the gradients either become vanishing small or tend towards infinity due to computational limitations\cite{vanishing_gradient_problem}, making training of the network impossible. LSTM architectures address this issue while keeping the network's ability to memorize long-term dependencies between data points in a sequence\cite{lstm_basics_one}.

Like all neural networks, an LSTM system must be trained on a large set of training data. 

\subsubsection{Sampling}\label{Sampling}

As explained in Section \ref{LM}, sampling describes the technique of how to select the next data point $\^x$, if the conditional probability distribution is given. Intuitively one might always choose the next data point which has the highest probability, as done in Section \ref{LM}. This method, however, has multiple pitfalls.

One of them is that it might result in endless loops like:
\begin{quote}
\textit{"to be or not to be or not to be ..." }
\end{quote}
The result of modern LMs putting great emphasis on the last few words of the context.

Another sampling technique could be randomly selecting the next data point from the probability distribution, called \textit{Random Sampling}. However, this might lead to a high chance of selecting meaningless words, mainly if the given vocabulary includes a high amount of words with a low probability. That is a result of the accumulated low probabilities of words that do not fit the context oftentimes is relatively high. 

To overcome these issues, modern sampling techniques are needed. One state-of-the-art method is \textit{Temperature Sampling}, which took its inspiration from the concepts of thermodynamics. 
In thermodynamics, a high temperature leads to an increased chemical reaction rate, speeding up sluggish reactions or making them possible in the first place. Conversely, a low temperature leads to reactions appearing less quickly. \textit{Temperature Sampling} takes this concept and transfers it to NLP. 
\paragraph{Temperature Sampling -}


In NLP, most neural networks create a raw vector as an output. To make sense of this output and the corresponding inputs, activation functions are needed\cite{activation}. To retrieve a probability distribution from an LM we need to apply the \textit{Softmax} function. \textit{Softmax} is a combination of multiple \textit{Sigmoid} and can be used for multiple dimension\cite{activation}, making it suitable for NLP. 
If \textit{Random Sampling} is used, the output of the \textit{Softmax} functions are used to sample randomly. Equation \ref{softmax} shows the \textit{Softmax} function\cite{activation}. In $\sigma(z_i)$, $(z_i)$ is a logit, therefore a non-normalized output vector of predictions \footnote{\url{https://developers.google.com/machine-learning/glossary\#logits}}. Output of the function is the probability of the $x_i\in X$, where again $X$ is the vocabulary.
\begin{equation}\label{softmax}
    prob(x_i) = \sigma(z_i) = \frac{e^{z_{i}}}{\sum_{j=1}^K e^{z_{j}}} \ \ \ for\ i=1,2,\dots,K
\end{equation}
If \textit{Random Sampling} the \textit{Softmax} function gets modified by a temperature parameter \textit{t} to create \textit{Temperature Sampling}, as shown in Equaiton \ref{temperature}\cite{temperature_sampling}\cite{temperature_equation_2}. Here the temperature \textit{t} gets used to scale the value of each logit, therefore each word, before transfering it to the \textit{Softmax} function. 
\begin{equation}\label{temperature}
    prob(x_i, t) = \sigma(z_i/t) = \frac{e^{z_{i}/t}}{\sum_{j=1}^K e^{z_{j}/t}} \ \ \ for\ i=1,2,\dots,K
\end{equation}
If the temperature parameter is less than one, it shifts the probability towards more likely events and lowers probabilities of already less likely events\cite{temperature_sampling}, overcoming the issue of \textit{Random Sampling}. Setting the temperature is a trade-off. If \textit{t} is close to $1$, the LM will create more diverse events but will improve generation quality if \textit{t} is set lower e.g $t=0.5$\cite{temperature_trade_pf}. 

\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{temperatur.eps}
  \caption{The probability distribution of the 100 most common words from the “I Have a Dream” Speech by Dr. Martin Luther King, Jr. (August 28, 1963)\footnotemark. Temperature Sampling has been done with $t=0,5$, leading to high probabilities being reinforced and low probabilities being dumped. For a better visibility, the x-axis does not display all 100 most common words.}
\end{figure}
\footnotetext{\url{https://www.loc.gov/static/programs/national-recording-preservation-board/documents/IHaveADream.pdf}}
\subsubsection{Evaluating a Language Model}
The quality of a language model depends on various factors, including the architecture of the neural network, the quality of the dataset, the syntax of the generated language, and more. Also, natural languages are very ambiguous, making NLG an error-prone process. Therefore, metrics are needed that evaluate the quality of given LMs. 
The most common evaluation metric for language models is \textit{Perplexity}\cite{perplexity}, shown in Equation \ref{perplexity}. \textit{Perplexity} can be seen as the level of uncertainty of the language model when predicting the next symbols in a sequence\cite{perplexityArg1}.
\begin{equation}\label{perplexity}
    PP(p)_T =  \sqrt[K]{\frac{1}{p(t_1,\dots,t_k)}} = \frac{1}{p(t_1,\dots,t_k)^{1/K}}
\end{equation}
To understand what \textit{Perplexity} is in detail, one must understand the goal of an LM. A LM has to assign high probabilities to "correct" sentences\cite{perplexity_good_description}. In this context, correct means that the sentences are represented as such in a data set. \textit{Perplexity} represents how perplexed an LM is if it tries to predict a (test) data set. If the \textit{Perplexity} is low, the model assigned a high probability to the data set itself and predicted it well. This can be seen by Equation \ref{perplexity}, where \textit{Perplexity} is defined as the average of the inverse probability $P$ assigned to each word $t_i$ in the (test) data set $T$\cite{perplexity}. 
\paragraph{Perplexity and Cross-Entropy - }
The \textit{Perplexity} is closely connected to the \textit{Cross-Entropy} of a LM. \textit{Cross-Entropy} is used to measure the dissimilarity of two probability distribution\cite{Cross-entropy}. In \cite{Cross-entropy} it is defined as: "...the average number of bits needed to encode data coming from a source with distribution p when we use model ...". Compared to \textit{Entropy} itself, which states the average number of bits needed to encode a random sequence from a probability distribution \cite{Cross-entropy}. \textit{Cross-Entropy} is defined as\cite{Cross-entropy}: 
\[ H(p)_T=-\sum_{t}^T p(t)\log_2p(t)\]
\textit{Cross-Entropy} and \textit{Perplexity} are monotonically related as shown in Equation \ref{perplexityAndCross}\cite{cross_entropy_and_perplexity}.
\begin{equation}\label{perplexityAndCross}
    PP(p)_T =  2^{H(p)_T} =  2^{-\sum_{t}^T p(t)\log_2p(t)} 
\end{equation}
\section{Approach}\label{Approach}
As mentioned before, the goal of this thesis is to create an application that automatically creates GUI designs. We will do so by leveraging existing NLG concepts and technologies. Details about NLG and its components have been covered in Section \ref{Background}.
NLG has been designed to create correct, understandable, and creative text in ambiguous human language using machine learning technologies and large data sets. It can do so by either creating a sentence randomly or by being given a seed as a starting point. For example, such a seed could be the beginning of a sentence like \textit{"to be or not to be}. 
Our system will do the same, but its inputs and outputs will represent a GUI. The challenge here is that NLG frameworks are designed to work with sequential data. Our approach is first to create a way to simplify a GUI into an abstract representation. This abstraction can then be preprocessed to sequential and language-like data. In the interest of more straightforward explanations, the abstract representation of a GUI will be called \textit{GUI-Abstraction}, and the preprocessed data used for LM training will be referred to as \textit{GUI-Language}. The \textit{GUI-Language} will then be fed to the machine learning application, creating a language model. Afterward, the language model can be used to create new GUI designs in the \textit{GUI-Language}, which can be translated into a \textit{GUI-Abstraction}. The whole approach is visualized in Figure \ref{Graph to LM}. 
\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{BA_Process_to_LM.jpg}
  \caption{To create the described LM, we first need a suitable dataset containing a large amount of GUIs, as described in Section \ref{The Data Set}. In the first step, the GUIs in the dataset will be simplified into the less complex and more structured \textit{GUI Abstraction}. Then, in the second step, the \textit{GUI Abstractions} will be flattened into text sequences, the \textit{GUI Language}. Lastly, we can use the \textit{GUI Language} to train the LM.}
  \label{Graph to LM}
\end{figure}
\subsection{The Data Set}\label{The Data Set}
A prerequisite for building such a system described is a suited and large enough data set of GUIs. 
For this, we chose RICO\cite{RICOStandard}.
RICO is initially available as a dataset for mobile app GUIs. It contains data from more than 9,700 Playstore apps across more than 25 categories, revealing visual, textual, structural, and interactive design properties of more than 72,000 GUI screens\cite{RICOStandard}. Within the RICO dataset, each GUI is represented by both a PNG file showing the GUI and a JSON file describing the view hierarchy of the GUI. The RICO dataset is sufficient in size for our application but is challenging to work with. The elements in the UIs are not classified, which will make it formidable to create a Vocabulary for our desired \textit{GUI-Language}. 
Further, the view hierarchy in the original RICO dataset is very granular in structure and information, which is detrimental to our work. In order to limit the scope of this thesis, a more simple GUI representation is required. Therefore, we will first focus on the dataset published in \cite{SemanticRICO}. The latter contains more than 66k UI screens from the RICO dataset, which have been simplified into a semantic replica that illustrates what the elements on the original UI screens mean and how they are used\cite{SemanticRICO}. It does so by classifying each component in the UI into one of 25 types and only including the originals component class and position in the semantic UI, as shown in Figure \ref{Rico semantic pic}. Exemplary component classes are: \textit{Icons}, \textit{Texts}, \textit{Text Buttons}, \textit{Images}, \textit{Inputs}, \textit{Toolbars} etc. 
\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{Graph - Rico -1.jpg}
  \caption{This image from \cite{SemanticRICO} shows the concept behind the semantified UIs in the used data set. The original UI gets abstracted, only leaving each elements class label and position within the UI.}
  \label{Rico semantic pic}
\end{figure}
Utilizing this semantic dataset will result in multiple benefits. First, the semantic UI only contains already labeled UI elements, saving us the effort to label them ourselves. Second, their view hierarchy is less complex, simplifying the data prepossess step. Third, if we aim to create our abstraction from the sematic UIs, it will allow us to only focus on the location and label of the UI elements. Therefore, we do not have to consider the UI elements' content, e.g., Text or Images, or particular functions, e.g., back or menu button. All together, making step 1 shown in Figure\ref{Graph to LM} feasible.  


Employing the semantic dataset does have its pitfalls, however. For example, the dataset contains GUIs from multiple domains, including mobile gaming UIs. Such UIs, especially in-game UIs, are often very different from regular and generic UIs, e.g., Login Screens. Therefore we will sort out UIs from the category "Entertainment" in the dataset from \cite{SemanticRICO}. 


Further, many GUIs from the dataset are from mobile apps that build their business on advertisements leading to many UI screenshots only being advertisements. While working with the dataset, we will filter these in Step 1. 


Lastly, the semantic UI representation from \cite{SemanticRICO} does not contain any information about the content of a GUI and its elements. It does only represent the general GUI layout. Using this dataset for Step 1, we limit our \textit{GUI Abstraction} to only containing sematic information about the GUI and later our \textit{GUI Language} to not have any pragmatic. Therefore, in the \textit{GUI Abstraction}, two elements from the same class, like an icon or image, will not have any difference except their location within the GUI. Therefore we will not differentiate between a "delete" or "update" icon or an image displaying a house or a shopping item.   
\subsection{GUI Abstraction}\label{Gui Abstraction}

After finding a suitable dataset, we can design the \textit{GUI-Abstraction} and start answering our research question R1 from Section \ref{INTRO}.

\subsubsection{Requirements}
The \textit{GUI-Abstraction} will later be used to create the \textit{GUI-Language} and is a vital part of this thesis. Therefore we created following requirements. 

First, it has to be able to be represented in a text sequence without the loss of any information; therefore, if we flatten the \textit{GUI-Abstraction} to the \textit{GUI-Language}, which is such a text sequence, the \textit{GUI-Language} has to contain all the information about the location and class of each element in the \textit{GUI-Abstraction}. 

Second, the  \textit{GUI-Abstraction} shall be able to catch the "sense" and the fundamental structures if reoccurring in reoccurring UI screens. For example, mobile applications share similar key concepts and processes like navigation via a menu panel or user management via accounts. Due to this nature, some generic UI screens are always reoccurring in different mobile applications like registration- and login screens or menu lists. Thus, the \textit{GUI-Abstraction} shall represent two different login screens so that the \textit{GUI-Abstractions} are similar to one other and have the same layout. This characteristic will enable the machine learning system to catch these reoccurring screens and later auto-generate them. 

Third, the \textit{GUI-Abstraction} must catch reoccurring screens while minimizing noise. Even though repeated UI screens, e.g., login screens, share a layout, the exact position size and number of elements may differ. The system, however, should not be confused by these dissimilarities. 

Fourth, in \cite{SemanticRICO}, 25 different UI element classes are defined, but not all classes are equally valuable for our purposes. Some classes are so-called \textit{Parents} in the UI and, therefore, in the view hierarchy as well. As already analyzed in \cite{GoogleHuang2021creating}, these parent elements do neither contain a lot of information nor are they interactive, and excluding them will result in less noise in the dataset and will make training easier.  
\subsubsection{Method:}
With the aim to fulfill all requirements above, we created the following \textit{GUI-Abstractions} illustrated in Figure \ref{Abstraction Example}. 
\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{PrÃ¤sentation5-2.png}
  \caption{Here, the abstraction process is illustrated. These are two examples from the RICO dataset. First, one can see the GUI screenshot. Then the semantic GUI is shown. Due to slight errors in the dataset from \cite{SemanticRICO}, the semantic representation had to be manually modified. On the right side, the resulting GUI Abstraction is shown. }
  \label{Abstraction Example}
\end{figure}

To abstract a GUI, a grid with three horizontal and two vertical lines is laid over the GUI screen. This grid splits the GUI vertically and horizontally into cubes. If the GUI is seen as a coordinate system, we can iterate over the x- and y-axis to assign coordinates $(x,y)$ to the different cubes. For example, looking at the GUIs in Figure \ref{Abstraction Example}, the cube at the top-left corner has the coordinated $(1,1)$. The cube at the bottom-right corner has the coordinates $(4,4)$. Now we can use the cubes and their coordinates to refer to different positions within the UI screen. 


Each element in the semantic UI can now be assigned to a cube. To do so, we first calculate the center point of each UI element. Then, laying the grid over the UI allows us to assign each element to its cube according to the location center point. For example, the abstraction will assign an Icon close to the right-top corner of the screen to the cube with the coordinates $(3,1)$. Likewise, if an Image covers the entire UI screen, it will be assigned to the cube with the coordinates $(2,3)$. If an element's center lies on a grid line, the cube with the higher coordinate is chosen. 

To create the \textit{GUI-Abstraction}, we iterate over every element and assign it to a cube as described. While doing so, we filter the parent elements and only include leaf elements. To further reduce noise in the dataset, we neglect the size of each UI element. Therefore, the \textit{GUI-Abstraction} only contains the coordinates, or the assigned cube, and the label of each UI component. Also, if multiple elements of the same class, e.g., two \textit{Icons}, are assigned to one cube, e.g., $(1,2)$, we won't count their quantity; therefore, the \textit{GUI-Abstraction} does not contain any information about the number of elements in the same cube with the same component class. Within each cube, the classes are sorted according to their number assigned in Table \ref{tabelle1}. 

\paragraph{Choosing Grid Finesse -}

As described, we choose a 3x4 grid while abstracting the GUIs. However,  the granularity reflects a trade-off between minimizing noise in the data set and accurately mirroring the layout of a GUI. A fine grid, e.g., 10x15, might catch each UI element's position accurately, but it will also make the dataset hard to train on since it will also catch slight differences. For example, between two different login screens with slightly differently placed buttons. On the other hand, a coarse grid will contain less information about the original layout of the UI. Still, it will reduce the dataset's noise and minimize differences between two similar GUIs. 

Thus, we decide to use a less fine 3x4 grid. This will allow the system to predict GUIs better and provide a proof of concept for our approach. 

\paragraph{Filtering UI-element classes -}

In \cite{SemanticRICO}, 25 different classes of UI-component are defined, as shown in Table 1. To identify which of these components are parent classes, one has to look into the view hierarchy of a UI. Paren classes are classes like List-Element or Multi-Tab. They contain little information and mainly serve to structure the GUI. For example, List-Element mainly contains additional child elements like Texts or Icons, which make up the List-Element. To identify the parent and leaf classes, I counted how often each class occurs across all GUIs in the dataset. Then I counted how often each class is a leaf element in the view hierarchy across all GUIs in the dataset. Classes that are either not represented as leaf elements or rarely occur as leaf elements are labeled as parent classes. 
\begin{table}[h!]
    \caption{After filtering our dataset contains 12 different classes of UI-Components. Here it is represented how often they occur withing the \textit{GUI-Abstractions} dataset, which will be use to train the LM.} \label{tabelle1}
    \begin{tabularx}{\textwidth}{|l|X|X ?l|X |X|}
    \hline
    Nr. & Class &  Count & Nr.&  Class &  Count\\
    \Xhline{3\arrayrulewidth}
    1& Text & 455075  & 7 & Background Image & 6726\\
    \hline
    2 & Image &  213775 & 8 &Radio Button & 5410\\
    \hline
    3 & Icon &  175861 & 9 & Pager Indicator  & 4144\\
    \hline
    4 & Text Button &  138834 & 10 & Checkbox   & 3893\\
    \hline
    5 & Input & 20809 & 11 & Slider  & 1895\\
    \hline
    6 & Web View & 19329 & 12 & Video & 491\\
    \hline
    \end{tabularx}
\end{table}
Looking at the dataset, we identified 12 different leaf classes and 13 parent classes. Table \ref{tabelle1} shows all leaf classes and how often they occur as leaves in the view hierarchies. 

\paragraph{Creating a new Dataset -}

I abstracted all GUI from the semantic RICO dataset with the method described in this chapter: Result is a new dataset containing view hierarchies of \textit{GUI-Abstractions}, which will be used for further preprossesing. While abstracting, we filtered out all GUIs that are advertisements or belong to the \textit{Entertainment} category. 

\subsection{GUI Language}\label{GUI Language}


We aim to utilize language model frameworks to create neural networks trained on the GUI Abstractions. However, those frameworks are only designed to work with sequential language-like data. Therefore we have to flatten the GUI Abstraction in the before-mentioned GUI Language. As a result, the GUI Language will be a text sequence and, therefore, usable for NLG. 


\begin{figure}[h]
\centering
   \includegraphics[width=\textwidth]{Beispiel_Language_Überarbeitet.pdf}
  \caption{This image illustrates how a GUI Abstraction gets flattened to the GUI Language. The GUI which is already shown as an example in Figure \ref{Abstraction Example} is represented as a string.}
  \label{Example_Language}
\end{figure}
We will design the GUI language as follows: 
Each element class will be represented by an according string. For example, an Icon will be represented by an $"ICON"$ string. 
Each cube in the abstraction will either be represented by such a string or by multiple strings concatenated with the symbol \textit{"\&"}. For example, the language will represent a cube containing an Icon and an Image element with \textit{"ICON\&IMAGE"}. If the cube does not contain any element, it will be represented with an \textit{"EMPTY"} string. 
Each row within the abstraction will be represented by multiple cubes separate with white space. So, for instance, one possible column could be \textit{"ICON EMPTY ICON\&IMAGE"}. 
A concatenation of its rows will represent each GUI. To symbolize the end of a row, $"|"$ is used. An example for one GUI with two rows could be:
\begin{lstlisting}
    "ICON EMPTY ICON&IMAGE | ICON EMPTY ICON&OMAGE"
\end{lstlisting}
The last step is to define the start and end of each sentence with a corresponding token. One complete sentence now looks like this:
\begin{lstlisting}
    "START ICON EMPTY ICON&IMAGE | ICON EMPTY ICON&IMAGE |
    EMPTY IMAGE ICON | ICON EMPTY IMAGE END"
\end{lstlisting}

The elements within a cube are sorted by their number assigned in Table \ref{tabelle1}. While creating the GUI language, we iterate over the rows from top to bottom, and within a row, we iterate from left to right cube. 

The GUI Language can be seen as a formal language over the vocabulary defined in Table \ref{tabelle2}. The syntax can be described with the extended Backus–Naur form (EBNF), while "{}" means an optional number of repetitions and "()" solely represents grouping. 

\begin{lstlisting}
    Sentence    = "START" GUI "END"
    GUI         = Row {"|" Row}
    Row         = " " Cube {" " Cube} " "
    Cube        = (Element {"&" Element}) | "EMPTY"
    Element     = "TEXT" | "IMAGE" | "ICON" |
                    "TEXT-BUTTON" | "INPUT" | "WEB-VIEW" |
                    "BACKGROUND-IMAGE" | "RADIO BUTTON" |
                    "PAGER INDICATOR" | "CHECKBOX" | 
                    "SLIDER" | "VIDEO".
\end{lstlisting}

\begin{table}[h]
    \caption{The GUI Language is a language over a vocabulary. Each word either represents an UI-element or has an special meaning.} \label{tabelle2}
    \begin{tabularx}{\textwidth}{|X|X|}
    \hline
    Vocabulary & Meaning\\
    \Xhline{3\arrayrulewidth}
    "TEXT", "IMAGE", "ICON", ... & Elements in a cube \\
    \hline
     "EMPTY" & Empty cube \\
    \hline
    "START" & Start of sentence \\
    \hline
    "END" & End of sentence\\
    \hline
    "$|$" & End of row\\
    \hline
    "\&" & Concatenation of elements\\
    \hline
    " " & End of cube\\
    \hline
    \end{tabularx}
\end{table}

\subsection{Training the system}\label{Training the system}
To train the machine learning application, we first preprocess the GUI Abstraction dataset, by flattening each abstraction into a text string as described in Section \ref{GUI Language}. 
While doing so, the text sequences corpus gets shuffled to destroy the order within the RICO dataset. 
An order resulting from the RICO dataset placing GUI-screens next to each other that are from the same app.

After filtering advertisements and faulty GUI view hierarchies, we resulted in a dataset of 64k GUIs, each represented by one string. 

We decided to implement a satet of the art character-to-character text generation model. It will be implemented utilizing \textit{Phyton} and the deep learning framework \textit{Keras}\cite{keras}. Section \ref{Implementation} gives a comprehensive overview of the implementation. 

Since the dataset is not officially partitioned, we decided on a 70/15/15 split of 70\% training data and 15\% validation and test data to train, validate and test the system. The initial shuffling,
as well as the sheer size of the corpus, guarantees that each split has a similar distribution of the 27 GUI categories. According to the Keras documentation \footnote{\url{https://keras.io/examples/generative/lstm\_character\_level\_text\_generation/}}, the training corpus should ideally have 1M tokens. Counting our train split, we identify 45k text sequences with a total of  1.7M tokens. 


To train the system, we feed each sentence independently to the LM. Starting with the "START" token, the LM predicts the next token from the given sequence. Then the predicted token gets compared to the actual next token to create conditional probabilities. This process is continued, token by token till the "END" token is reached. A method called teacher forcing\cite{Goodfellow-et-al-2016} and is illustrated in Figure \ref{Teacher Forcing}. 

\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{Teacher Forcing.pdf}
  \caption{Here, teacher forcing over three time steps is illustrated. The model predict the next token in a sequence. Regardless of the prediction, the ground truth is always used as a input to predict the next token. The ground truth in this image is "START Icon\&Text Text ...". The model predicts however, "START Image TextButton\&Text Empty".}
  \label{Teacher Forcing}
\end{figure}

After each training cycle, the LM gets validated using the test dataset. Then, we set the aim of the model to minimize the Perplexity of the LM and adjust parameters accordingly. 
This process gets repeated over multiple epochs until the best epoch is found. 
Details on the training process can again be found in Section \ref{Implementation}. 

After training our model, we received a working LM that can generate conditional probabilities over the GUI Language. The next step is to use this model for \textit{GUI Language} generation. 

\subsection{Generate Language}\label{generate Langauge}

After creating the LM and training it, we can use it to generate new UIs. To do so, we can go with two different approaches. 

The first approach will be similar to the training of the model. We will give the model a text sequence, which the model shall predict token by token, starting at the first token. After predicting one token, the model will be given the ground truth token as additional context to predict the next token again. Therefore, this approach relies on us feeding the system the entire GUI before prediction and is not practical. However, this way of NLG is valuable to get insights into the model's predictions and helpful in debugging and analyzing. 

The second approach will be to use the output of the LM as the input for further predictions. The LM will be given an initial text sequence called "seed".
 The model will predict the next token in the sequence based on the seed. The predicted next token will then be added to the seed and used as a new input for the generation of the next token. This process is done recursively until the LM generates the "END" token to symbolize the end of the predicted GUI. The initial seeds can vary in length. The longer the seed, the more context is given, leading to a better prediction of the GUI. However, the challenge is to predict well with little context. 
Valid seeds could be: 
\begin{lstlisting}
    S1: "START"
    
    S2: "START ICON EMPTY ICON&IMAGE |"
    
    S3: "START ICON EMPTY ICON&IMAGE | 
        ICON EMPTY ICON&IMAGE |"
\end{lstlisting}
S1 is only the start token; therefore, the model generates GUI without any context. 
S2 represents only the first row of the GUI and gives 25\% of the GUI as context. 
S3 represents the first two GUI rows and 50\% of the GUI as a context. 
The model is expected to perform better on S3 than on S2. The reason is that we use the model's output as the basis for further text generations. Therefore, if the output is faulty, the model will create predictions based on these errors resulting in a snowball effect. On S1 the model will predict a random GUI. The process of recursive prediction with S2 is shown in Figure \ref{Seed Generation}.
\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{Seed_Gen.pdf}
  \caption{Here, recursive text generation over three time steps is illustrated. Based on an initial seed in t1, the model predict the next token. The prediction is then used as a input to predict the next token. Continuing this process will result in the LM predicting the entire GUI.}
  \label{Seed Generation}
\end{figure}
\subsubsection{Sampling}
As discussed in Section \ref{INTRO}, the LM only generates a conditional probability distribution over the given vocabulary. To predict tokens or entire sequences, as shown in Figure, we need a sampling method. Therefore we need to select the next token based on the given probability description. As a sampling method, we use temperature sampling described in Section \ref{Sampling}. 
The temperature parameter $t$ from Equation \ref{temperature} can be chosen according to the desired outcome. For example, if the model shall predict the GUIs precisely and shall continue to build on the already existing layout pattern, a low temperature should be selected e.g., $t=0.2$. 


If the model must predict GUIs diversely and more "creatively" the temperature should be high, e.g., $t=1$. In that case, the GUI Layouts will less likely follow existing patterns. 
We will test the model with different temperatures during the evaluation in Section \ref{Evaluation}. 
\subsubsection{Represent LM Outputs}
So far, our system only outputs text sequences in the \textit{GUI Language}. These are readable and understandable but challenging to grasp. For example, looking at sentence S1 below, it is not easy to imagine how the resulting GUI Abstraction is built. To make the result more intuitively understandable, we created an alternative structure to the \textit{GUI Language}. 

We will refer to this alternative structure as the pretty print.
One can also interpret the pretty print as another way of displaying the GUI Abstraction. An example of the pretty print is shown in Figure \ref{Pretty Print}.
\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{Pretty Print 2.0.pdf}
  \caption{An illustration of the pretty print with already know \textit{GUI-Abstraction} and \textit{GUI-Language} example. The pretty print is structured in a grid, like the \textit{GUI-Abstraction}. Therefore the pretty print is more easy to understand than the GUI Language. Each cube within the pretty print either contains the elements of the abstraction, or an "Empty" string to represent an empty cube.}
  \label{Pretty Print}
\end{figure}

\subsubsection{Testing the Language Generation}

To test the language generation, we will go two ways. 


First, we will evaluate the LM text generations quantitatively. Here we will exclusively use the metric \textit{Perplexity}, defined in Section \ref{Modern Approach to LM}.
To test the \textit{Perplexity} of the system, we will iterate over the test data set, containing 9548 example GUIs. One GUI is represented in the \textit{GUI-Language} as $(t_1,...,t_n)$.
The system will predict the probability $p$ of each text sequence, token by token. At each time step $t$, the model predicts $p(t_1,...,t_t)$, and the \textit{Perplexity} according to Equation \ref{perplexity} is calculated. 
To get the system's final \textit{Perplexity}, we will average over the  \textit{Perplexity} at each time step.


We aim for a low \textit{Perplexity} with our system. If the LM manages to predict the ground truth at every time step, it will result in a \textit{Perplexity} of one. However, since this is neither realistic nor do we have any references for good perplexities, we also rely on an alternative evaluation approach. The problem is that our approach is very novel and that we not only created a new way to abstract GUIs for NLG, but we also created a new language to be able to process the abstraction. 
Therefore, we do not have related works or models to capture how our model compares to them. We can aim for a low \textit{Perplexity} but cannot evaluate which numeric value we have to reach to regard our system as well-performing or not. However, \textit{Perplexity} still offers the best way to optimize our system during training and validation by minimizing this metric. 

Alternatively, we evaluate the system qualitatively. We will let the system predict GUIs via the recursive text generation method described in Section \ref{generate Langauge}. 
This will be done with different seeds with different lengths and from different GUIs from the test data set. Eventually, the generated outcome can be compared to the original \textit{GUI Abstraction}. Here we can analyze how well the system captures existing structures within a GUI, how well the system predicts reoccurring screens like login pages, and which GUIs the system cannot predict well. On top of this work, flaws and well working parts of the system can be found.

\section{Implementation}\label{Implementation}
The implementation of our system was done via phyton and structured in three different sub-projects, the \textit{Abstractor}, the \textit{Preprocessor} and the \textit{Language Generator}. Each sub-project is independent of the others and fulfills one use for our system. 
At this point, I have to make my first acknowledgment to Mr. Kolthoff from the Institut for Enterprise Systems at the University of Mannheim. This implementation was done on top of various helper functions and already existing classes created by him. In Section \ref{FurtherInfo}, access to the implementation can be found. 

\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{Advertisement_GUIs_2.pdf}
  \caption{Here two GUIs are shown, that get labeled as advertisements during abstraction. As one can see both elements consist mainly out of one Web-View element. }
  \label{Advertisemnt}
\end{figure} 

\subsection{Abstractor}
The first part of the implementation is the abstractor. This tool is designed to read the semantic RICO dataset\cite{SemanticRICO} and abstract it as described in Section \ref{Approach}. 


The tool has settings enabling modifying the number of columns $X$ and rows $X$ of the grid in GUI Abstraction. During the entirety project, $X$ was set to $3$, and $Y$ was set to $4$. 
The tools workflow is as follows. 
It iterates over every \textit{.json} view hierarchy in the semantic RICO dataset.


While doing so, the program recursively collects all leaf elements in the view hierarchy. For every leaf, the coordinates in the abstraction are getting calculated. The tool does so by looking at the bound of the UI element and calculating the element's geometric center. Then, the element gets placed in the cube of the abstraction by that center's location. After gathering all leaf elements and their coordinates, a new \textit{.json} represents the GUI abstraction. The new \textit{.json} contains the number and classes of leaf elements for every cube. 


In parallel, the tool iterates over a second dataset published in \cite{RICOStandard} that contains additional meta information about the GUIs. From this dataset, we read the category of the GUI. 


In a second step, each GUI gets checked for being an advertisement or not. Since the Dataset does not define which GUI is an advertisement or not; we have to label the GUIs ourselves. We defined all GUIs that either only contain one \textit{Web View} element or one \textit{Web View} and one \textit{Icon} or \textit{Advertisement} element as advertisements; in Figure \ref{Advertisemnt} GUIs that fulfill this criteria can be seen. Even though this labeling method is very elementary and results in false positives as well as false negatives, it filters out the most superficial advertisement like GUIs, which is sufficient for our purposes.

Finally, we bundle the GUIs category, advertisement classification, and the number of elements as metadata to the created \textit{.json}. 
The resulting \textit{.json} is illustrated in Figure \ref{tree}. 
\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{tree_png.png}
  \caption{Here one can see the \textit{.json} created by the \textit{Abstractor} for the already known Example 2 from Figure \ref{Abstraction Example}. It is illustrated as a tree diagram created by an online tool\footnotemark{} from the GitHub User \textit{van111}. The nodes 0, 1, 2 \& 3 with the depth of 1 represent the rows of the abstraction. Each leaf represents one cube. The metadata node contains information that is useful for later steps.}
  \label{tree}
\end{figure}

\footnotetext{\url{https://github.com/ivan111/vtree\#readme}}

\subsubsection{Exception Handling}
During the aforementioned abstraction process, we had to filter out faulty GUIs by two different reasons. 
First, we filtered GUIs where the bounds of the GUIs-leaf elements were not within the bounds of the GUI itself. 
Second, we filtered out GUIs where the bounds of the GUI were wrongly defined in general. For example, this was often the reason if the width or height was zero. 

\subsection{Preprocessor}
The second part of the implementation is the \textit{Preprocessor}. It is a tool that preprocesses and filters the GUI Abstractions the Abstractor created into the GUI Language. 
It works by iterating over the abstracted GUI dataset. Every \textit{.json} then gets translated into a string. The resulting set of strings is then shuffled to create an even distribution of GUI categories as described in Section \ref{Training the system}. 70\% of the resulting strings are saved in the training dataset, 15\% in the validate dataset, and 15\% in the test dataset.
During this step, the \textit{Preprocessor} filters out GUIs that are disadvantageous for training our LM. Therefore we filter out all GUIs belonging to the category Entertainment and all GUIs that have been labeled as advertisements, as explained in Section \ref{The Data Set}. This can fastly be done by using the metadata node in the .json 

The resulting data sets contain strings in the \textit{GUI Language} discussed in \ref{GUI Language}. Even though not easy to grasp at first, these strings are generally readable by humans. It is so because the language vocabulary contains words that intuitively explain what they represent, e.g., "START", "ICON". 
Beneficial for us, this will make the dataset more challenging to work with during language generation. 

To simplify later steps, we will translate these readable strings into a more compact version. 

A single char will replace each word within the vocabulary of the \textit{GUI Language}. Since we have 12 component classes, each having a number assigned in Table \ref{tabelle1}, we will assign each class its number in hexadecimal. The "Empty" token will be replaced by the number 0, and "START" and "END" will be replaced with the chars "$<$" and "$>$". The resulting string will be referred to as \textit{Char String}, while the already existing readable string will be called \textit{Readable String}. 

An example translation is shown below:
\begin{lstlisting}
    - Readable String: "START Icon&Text Text TextButton | 
        Empty TextButton Empty | Empty TextButton Empty | 
        Empty Text Empty END"
    
    - Char String: "< 1&3 1 4 | 0 4 0 | 0 4 0 | 0 1 0 >"
\end{lstlisting}

Finally, the \textit{Char String} are the data fed to the LM. 

\begin{table}[h]
    \caption{To create the \textit{Char String}, we assign each token to a single char. The tokens and corresponding chars are shown in the table. } \label{tabelle3}
    \begin{tabularx}{\textwidth}{|X|X|}
    \hline
    Vocabulary & Char Representation\\
    \Xhline{3\arrayrulewidth}
    "TEXT", ..., "VIDEO" & "1",..., "C" \\
    \hline
     "EMPTY" & "0" \\
    \hline
    "START" & "$<$" \\
    \hline
    "END" & "$>$" \\
    \hline
    "$|$" & "$|$"\\
    \hline
    "\&" & "\&"\\
    \hline
    " " & " "\\
    \hline
    \end{tabularx}
\end{table}

\subsection{Language Generator}
The \textit{Language Generator} is the last sub-part of the implementation. The neural network first gets trained on the GUI strings created by the \textit{Preprocessor} and then predicts its own text sequences. 
The \textit{Language Generator} will be implemented via the deep learning framework \textit{Keras}\cite{keras}. We can use Keras to create an LSTM neural network quickly and therefore follow a state-of-the-art approach to language modeling.

However, typical language models are trained on a large corpus of text, often represented as a single text file. 
The model iterates over the text corpus $t_1, ...,t_m$ with a text input of a fixed length n. It starts predicting the token at the position n+1 in the corpus, with tokens $t_1, ...,t_n$ as an input. After predicting for the first time it uses $t_2, ...,t_{n+1}$ as the next input. It continues to do so at each timestep till it predicts $t_m$.

Nevertheless, this traditional approach does not work with our system. 
Even though our data set of GUI strings is stored in one single text file, it shall not be interpreted as one single document. Each string representing one GUI must be regarded on its own. If we follow the traditional approach and iterate over the whole corpus, we might regard context from a previous GUI to predict the current GUI. Such is to avoid since the conditional probability of every element within a GUI is only dependent upon previous elements in the same GUI. 


Also, we cannot set the input length n randomly. To predict GUIs syntactically correct, the LM needs to know when to set structural tokens like "$|$" or "$>$". For example, if abstracted into a $3x4$ grid, every GUI row ends after three cubes, and every GUI ends after three rows. If n is not big enough, the LM cannot grasp the entire GUI, especially the start token, as a context and is not able to predict these structural tokens correctly. As a result, the LM might be caught in an endless loop and never set the end token or might end predicting too early. 

Therefore we have to set n large enough so that the LMs input always includes all previous tokens within the GUI and no context is getting cut out. 

To solve this problem, our model will be trained on one GUI at a time. As explained in Section \ref{Training the system}, we will train the model via teacher forcing. It will start predicting at the second token until the end token. We set the fixed input length to the max length of a Char String recorded in the dataset, 87. This means that the longest GUI Language string contains 87 tokens, which is the upper length limit for our system to predict. 
Since each input string for our LM now has to have a length of 87, we have to utilize padding. Each input string to our LM will be padded to this length with the char "\_". 
The system is set to recognize this char as padding and ignore it for predictions. 

\subsubsection{Neural Network Structure}

The neural network is build via Keras and contains four different kinds of layers: 
\begin{itemize}
    \item The Embedding Layer 
    \item The LSTM Layer 
    \item The Dropout Layer 
    \item The Dense Layer 
\end{itemize}

Each will be shortly described.
\paragraph{The Embedding Layer -}
The embedding layer is the input layer in our network and transforms the tokens into \textit{Embeddings}. 
In .\cite{embedding}, \textit{Embeddings} are defined as Vectors that represent words. More often, word embedding refers explicitly to a representation via a dense vector with real values \cite{embedding}. Therefore each word can be assigned to a position in a Vector space. This position also represents the semantical meaning of the word leading to similar words gathering close to each other in the vector space\cite{embedding}. 

According to the Keras source code, the embedding layer transforms inputs into dense vectors of a fixed length\footnote{\url{https://github.com/keras-team/keras/blob/v2.9.0/keras/layers/core/embedding.py\#L32}}. This length is called embedding dimension, a hyperparameter for our neural network. Generally, we can expect a relatively low embedding dimension to work sufficiently since our vocabulary of 16 tokens is fairly small. 

Also, Keras native embedding layer offers an automotive masking function. We make use of this function to handle our padded sequences accordingly. 

\paragraph{The LSTM Layer -}
The LSTM layer is a hidden layer in our neural network and is the core component. 
The basic idea behind LSTM was described in Section \ref{Modern Approach to LM}. Like every neural network, an LSTM layer is made of multiple units, often called neurons. 
While building our neural network, we have to decide on two hyperparameters regarding the hidden layers. The first is the number of LSTM layers within our network, and the second is the number of neurons for each LSTM layer. The Keras implementation of LSTM can be found on GitHub\footnote{\url{https://github.com/keras-team/keras/blob/v2.9.0/keras/layers/rnn/lstm.py\#L347}}. 
\paragraph{The Dropout Layer}
The Dropout Layer is the second hidden layer within our neural network. It aims to reduce overfitting, therefore to "contain more unknown parameters than can be justified by the data"\cite{Overfitting}. If our model is overfitted, it is trained to accurately predict the training data by being specialized on the single data points. It does not generalize anymore and loses the ability to predict the future so far unseen data well. Overfitting often occurs if models contain a large number of parameters and are trained over too many epochs, resulting in worse and worse performance on validation and test data. This is often identified by comparing training and validation error. If the training error decreases with ongoing epochs, but the validation error has stagnated or has already reached its global minimum, the model is overfitted. 

According to the Keras source code, the dropout layer "randomly sets input units to 0 with a frequency of \textit{rate} at each step during training time" \footnote{\url{https://github.com/keras-team/keras/blob/v2.9.0/keras/layers/regularization/dropout.py\#L26}}.
A state-of-the-art approach to reduce overfitting by preventing co-adaptions of the network layers on the training data\cite{dropout}. By randomly dropping out neurons, the layers function differently at every step, making following layers challenging to adapt to prior mistakes; a process eventually forces more generalization of the layers\cite{Overfitting_explained}. We add a dropout layer after every LSTM layer and set the dropout rate to 0.2. 

\paragraph{The Dense Layer}
The dense layer is the output layer of our network. In general, a dense layer is a closely connected layer that receives inputs from every unit of the preceding layer. Through matrix multiplication, the Vectors get flattened into one dimension. 
Also, Keras native embedding layer offers an automotive masking function. The Keras implementation of a dense layer can be again found on GitHub\footnote{https://github.com/keras-team/keras/blob/v2.9.0/keras/layers/core/dense.py\#L32}. Through the activation with the Softmax function in Equation \ref{softmax}, we can transform the output vector into the desired probability distribution over our vocabulary. 

\subsubsection{Loss Function}
We have to define a loss function to be able to update and optimize our model during training. The loss functions can be interpreted as feedback for the neural network. With the aim of optimizing the loss metric, the system's parameters can be updated. 
Since we want to achieve a low perplexity of our system, it is intuitive to use \textit{Perplexity} as a loss function. 
As shown in Equation \ref{perplexityAndCross}, the \textit{Perplexity} and the \textit{Cross-Entropy} are monotonically related, which means that when the \textit{Perplexity} function reaches its global minimum, \textit{Cross-Entropy} does so as well. 
Therefore we chose to leverage this relation by using cross-entropy as a loss function since it is natively implemented in Keras. 

Lastly, we have to implement an optimizer into our network. Optimizers utilize the loss function to update the parameters of the neural network. We choose to implement the \textit{RMSprop} optimizer\footnote{\url{https://github.com/keras-team/keras/blob/v2.9.0/keras/optimizers/optimizer\_v2/rmsprop.py\#L28}} implementing the RMSprop algorithm\cite{RMS}. As a prerequisite, we are required to set a learning rate which will be our last hyperparameter. 
The learning rate indicates, how by much the models parameter are getting changes in response to the measured loss. 

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.7\textwidth]{Neural_Network_Final.eps}
  \caption{Our final neural network consist out of one input and output layer, and two LSTM as well as dropout layers. Each LSTM layer contains 128 neurons.}
  \label{Network}
\end{figure}
\subsubsection{Hyperparameter Tuning}
Before, three different hyperparameters were described. The \textit{embedding dimension}, the \textit{number of LSTM layers}, the \textit{number of neurons} in each LSTM layer, and the \textit{learning rate} of the optimizer. Now we have to find for each hyperparameter the value for which the system performs the best. Therefore, the value with which the system has the lowest \textit{Cross Entropy}. 

To find the best hyperparameters, we implement the hyperparameter optimization framework KerasTuner\cite{kerastuner}. Then, we set the KerasTuner to find the best hyperparameters via the Hyperband algorithm \cite{hyperband}.

As an input, we need to provide a range of values each hyperparameter can have. Using the chosen algorithm, the frameworks then select the combination of hyperparameters that reduces the validation loss the most over a set amount of epochs. 

We define the possible values of the hyperparameters as follows: 
\begin{lstlisting}
    - Embedding dimension   = {8, 32, 64}
    - Number of layers      = {2, 3}
    - Number of neurons     = {64, 128}
    - Learning rate         = {0.01, 0.001, 0.0001}
\end{lstlisting}

We set the maximum amount of epochs per hyperparameter configuration to 10. 
Due to computational limitations we were limited in the number of neuro
Due to computational limitations, we were limited in we decided to set the upper limit for the number of neurons and layer as shown. The computation was done on an M1 8-Core GPU. After about four days of computation, the program terminated, and KerasTuner determined the best configuration to be: 

\begin{lstlisting}
    - Embedding dimension   = 32
    - Number of layers      = 2
    - Number of neurons     = 128
    - Learning rate         = 0.001
\end{lstlisting}

With this configuration the network reached a \textit{Cross Entropy} validation loss of about $0.4899$. Therefore our network will look like as shown in Figure \ref{Network}

\subsubsection{Model Training}
After determining the hyperparameters to use, we need to determine the number of epochs for which the model should be trained.
To do so, we let the model train and validate on the dataset with a maximal amount of epochs and track the validation loss after every epoch.
Since we are bound to limited computation resources, we implement a callback function to stop training if the validation loss does not decrease significantly after a set amount of epochs.  
The program will terminate after the callback function gets called or the maximal amount of epochs is reached. 


We set the maximal amount of epochs to 30 and the batch size to 128. 
The program terminated after 16 epochs and 18 hours of computation, with the validation loss fluctuating around its global minimum of about $0,485$ after nine epochs. 
The validation and test loss can be seen in Figure \ref{loss_epoch} and the models training and validation accuracy is shown in \ref{accuracy_epoch}.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{Graph_Loss.png}
  \caption{Here the training and validation loss is shown. The values given have been rounded to two decimals for easier readability. As one can see, the validation and training loss improvement is decreasing over the amount of epochs. Also no over-fitting effect on the training data is shown, as expected after implementing dropout layers into our neural network.}
  \label{loss_epoch}
\end{figure}


\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{Accruracy Training.png}
  \caption{Here our models training and validation accuracy is shown, showing a similar pattern to the cross-entropy loss in Figure \ref{loss_epoch}. Again, the improvement is significant for the first iterations, with stagnation towards the end.}
  \label{accuracy_epoch}
\end{figure}

\subsubsection{Final Model for Evaluation}
Since the model from Figure \ref{loss_epoch} and Figure \ref{loss_epoch} shows a good perplexity and accuracy after the training process, we decide to use this model for the evaluation of the GUI generation approach.

The final model was trained over 16 epochs with the hyperameters defined during hyperparameter tuning and a structure as shown in Figure \ref{Network}. It has about 217k trainable parameters, 0.5k in the embedding layer, 82.5k in the first LSTM layer, 13k in the second, and 2.5k in the output layer. 


\section{Evaluation}\label{Evaluation}
So far, this work has proposed an approach to abstract and flatten GUIs into sequential data. Further, a state-of-the-art neural network has been created and trained to implement the approach and create an LM over the sequential GUI. Therefore research question R2 has been answered. 
In this chapter, we want to evaluate the language model and answer research question R2: How well is our approach suited to generate GUIs automatically?

We will evaluate the model first by its Perplexity, which is a state-of-the-art metric to define the performance of an LM. Also, we will take a look into the Accuracy of the model. However, as already discussed in Section \ref{generate Langauge}, those metrics can only give us limited insides into the abilities and downsides of our model. Therefore we will also manually analyze the generations of our GUI and compare them to the GUIs in the dataset. 

All values in this Section have been rounded to four decimals to support readability. 
\subsection{Performance by Metrics}

During the data set creation, we set 15\% of the GUI data aside to test the system. This set $T$ totals 9448 GUIs with about 380k characters. We let the model predict the test set with a batch size of 128. The used approach was again teacher forcing. 
\subsubsection{Perplexity}
The system archieved a Cross-Entropy of $0.4880$ during testing. Using Equation \ref{perplexityAndCross}, we are able to calculate the Perplexity of the model. The Cross-Entropy $H(p)_T$ is traditionally calculates like shown in Section \ref{Modern Approach to LM}, as: 

\[H(p)_T=-\sum_{t}^T p(t)\log_2 p(t)\]
However, one must know that the Keras framework runs on top of Tensorflow\cite{tensorflow2015-whitepaper}, a machine learning library. The native log function of Tensorflow uses e as a basis\footnote{\url{https://www.tensorflow.org/api\_docs/python/tf/math/log}}. Therefore Tensorflow calculates Cross-Entropy differently as: 

\[\^H(p)_T=-\sum_{t}^T p(t)\ln p(t)\]
Leading to $\^H(p)_T$ and $H(p)_T$ being offset by a factor of $\log_2 e$:

\[\^H(p)_T=H(p)_T*\log_2 e\]
Finally, we have to modify Equation \ref{perplexityAndCross} by changing the base to $e$:

\[PP(p)_T=e^{\^H(p)_T}\]
That results in a Perplexity of $1.6291\approx e^{0.4880}$. 
However, interpreting this result is difficult, as already explained in Section \ref{generate Langauge}. Problematic is that our approach is very novel, and we created the text corpus our model predicts by ourselves. Therefore, we do not have related works or models to capture how our model compares to them. On top, it is difficult to judge how ambiguous our text corpus in the \textit{GUI-Language} is. More ambiguity leads to a more perplexed system and, therefore, to worse performance.

The \textit{GUI-Language}s vocabulary consists of 18 different tokens. Therefore the highest, and worst, perplexity could have been 18 if the model assigns each token the same probability at each time step. On the other hand, the best performance would have been if the model assigned the highest probability to the test set. 

One could argue that the perplexity of 1.6291 is very close to the top value of 1.  However, the vocabulary used is rather small in size. Also, the GUI-Language has a lot less ambiguity than normal human languages. It is very structured with tokens like \textit{"END"}, "$|$" and often \textit{" "} being at the same location in every sentence. All this makes the \textit{GUI-Language} easier to predict. In contrast, one could argue that despite these circumstances, a perplexity of 1.6 is exemplary, especially when predicting creative work such as GUIs. 
\subsubsection{Accuracy}
Perplexity is a difficult metric to grasp intuitively but representative of an LM's performance if able to be benchmarked. Accuracy on the other hand, is an intuitive metric. Our model achieved an Accuracy of 0.8109 during testing. Therefore, it predicts the correct token in about 81\% of all cases. Even though this seems like a good rate, it should be put in relation. 

For example, a primitive model that would predict an empty GUI every time would achieve an Accuracy of about 49\%, if using teacher forcing. The reason is that the mole would always predict a string like shown below. 

\begin{lstlisting}
    Empty GUI:      "START Empty Empty Empty | 
                    Empty Empty Empty |
                    Empty Empty Empty | 
                    Empty Empty Empty END"
            
    Non-Empty GUI:  "START Icon&Text Text TextButton | 
                    Empty TextButton Empty | 
                    Empty TextButton Empty | 
                    Empty Text Empty END"
\end{lstlisting}

If compared to a not empty GUI, one can see that the model would always predict all spaces directly after the \textit{"START"} tokens correctly. Also, it would predict all \textit{"Empty"} tokens, as well as spaces after empty tokens. Further it would predict all spaces, after cubes correctly, that only contain one element like \textit{"TextButton"} or \textit{"Text"}. Finally, it would predict all \textit{"$|$"}, \textit{"Empty"} and spaces after \textit{"$|$"} correctly. Since the model always starts to predict after \textit{"START"}, we can neglect this token during calculations. 

The entire test dataset, has 9448 \textit{"END"} tokens, $9448*3=28644$ \textit{"$|$"} tokens, 89336 cubes that only have one or no element and 53117 of which are empty.  
Summing up the correctly predicted tokens of the entire dataset, the primitive model would predict 180645 tokens correctly. Minus \textit{"START"} tokens, the corpus is made out of 370768 tokens, leading to a accuracy of 0.4872, as mentioned above. 

Our model is, therefore, 1.6644 times more accurate than the primitive model. 

\subsection{Analysing GUI Predictions}
In this subsection we will analyse the predictions of our model qualitatively. To do so we first have to analyse the RICO dataset briefly.

\subsubsection{Overlook of RICO}

The RICO dataset contains 70k+ GUIs. All of them are individually distinguishable. We created the GUI-Abstraction and utilized it to process the semantic dataset. Our abstraction aimed to create patterns within the dataset. Therefore to reduce noise in the dataset and enable the neural network to recognize reoccurring GUI layouts, as described in Section \ref{Gui Abstraction}. To determine if the model is able to recognize and predict these patterns, we first have to identify them by ourselves.  

One of the most reoccurring GUI patterns with RICO are GUIs made mainly of one Element, the web view element. Those GUIs are often applications that open a website that make up most of the screen. Further, those GUIs mostly include a toolbar. An example is shown in Figure \ref{WebView-Example}.

Another reoccurring pattern is a list-like structure. They are made of similar vertical list elements stacked horizontally, with few other elements. Common list types include side bars, menu screens, and registration/login screens. Most often, their list items are made out of icons or images placed next to a text element. Other variants are made out of Text-Buttons or Inputs. Example list-like GUIs are illustrated in Figure \ref{List-Example}

Alternative structures are grid structures. They are made out of combined square elements placed in a grid. Common types of grid structures are shopping screens, calculators, or category selections. Most often, the square elements are either made out of Text-Buttons, or Images with a Text next to them, like shown in Figure \ref{Grid-Example}.

The most consistent element throughout all GUIs is the Toolbar. More than 50\% percent of all GUIs do have one. It is placed on top of the screen containing icons on the sides and a text element in the middle. 

\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{WebView-Example.pdf}
  \caption{RICO contains a significant amount of GUIs mainly made out of one central WebView element.}
  \label{WebView-Example}
\end{figure}

\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{List-Example.pdf}
  \caption{List-like GUIs are very common in the RICO dataset. They can contain various horizontal list elements, including Icons, Images, TextButtons, and more..}
  \label{List-Example}
\end{figure}

\begin{figure}[h!]
\centering
   \includegraphics[width=0.7\textwidth]{Grid-Example.pdf}
  \caption{Just like list-like GUIs, grid-like GUIs are very common. They often appear within the context of online shopping, category selection, and more.}
  \label{Grid-Example}
\end{figure}

\paragraph{Noise in the Dataset -}

The semantic dataset in \cite{SemanticRICO} was used for our GUI-Abstraction. Even though RICO is the current state-of-the-art for Mobile App GUI datasets, such datasets inevitably contain a degree of noise. Moreover, the semantic annotations were conducted via a heuristic approach leading to inconsistencies. 

For example, the heuristics seem unable to separate an icon from an image clearly. The annotation between small icons or images of the same size seems somewhat random, especially in list GUIs.  
Equally difficult is the separation of elements such as Text, TextButons, and Inputs. In contrast to the problem of images and icons, these elements are mostly consistent within a GUI. Between two different GUIs, however, the labels seem to vary. For example, if two different GUI sidebars consist of buttons with a text and an icon, the elements of one GUI could be labeled as TextButtons and those of the other as Texts and Icons.

Other inconsistencies include but are not limited to: Randomly leaving out single elements and not annotating them;  Sometimes including background elements and sometimes leaving them out; Periodically including elements that are either covered by other elements and not seeable or out of the GUI bounds. 



\subsubsection{GUI Generations without Context}

Our model can create GUIs with or without given context. If generated without or only minimal context, the resulting GUIs are mostly freely created without the possibility to steer the outcome. We can only direct the diversity of the GUIs by changing the temperature parameter during sampling. We limit the selection of our temperature parameters to 0.2 and 0.5 since these values show the best outcome and allow us to choose to have more or less diversity. 
%\paragraph{Generations with no Context -}
If our model creates GUIs with only the "START" token given, the results vary. One can see the resulting abstractions in Figure \ref{Free Generatiopns Compact} in the pretty print format. 

\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{Free Generatiopns Compact.pdf}
  \caption{Here are four GUI abstractions freely generated by our model. All four predictions represent a prevalent structure in the data set.}
  \label{Free Generatiopns Compact}
\end{figure}

The generated GUI "Generation 1" from Figure \ref{Free Generatiopns Compact} is very similar to the abstraction of grid GUIs in Figure \ref{Grid-Example}.
The first row of the abstraction indicates a toolbar, by including an Icon and/or Image in cube (1,1) and (3,1). Combine with the Text element in (2,1), this is a very reoccurring structure if a GUI is having a Toolbar. On top, all cubes do contain an Text and Image element and is therefore very similar to he example in Figure \ref{Grid-Example}. 

The generated GUI "Generation 2" from Figure \ref{Free Generatiopns Compact} shows parallels to "Generation 2". It does also indicate a toolbar at the top of the screen. Instead of a grid structure, a list structure of Texts was generated as shown in the example from Figure \ref{List-Example}. 

This behavior can easily explained by the generation approach as well as the dataset. For the first row the model almost always generates a toolbar. As explained, toolbars often consist out of a Icons on the left and right side and a Text in the middle. Due to the heuristic annotations, the Icons often get annotated as an Image. 
For the following rows, the model sticks to the pattern generated in the first row, resulting most of the time in a list-like or grid-like GUI.  
 
Changing the temperature to 0.5 slightly changes the behavior of the model. Most of the time the resulting abstraction is very structured, only including the four most common element, Icons, Images, Texts and rarely TextButtons. An example is given with "Generation 3" from Figure \ref{Free Generatiopns Compact}. 

Sometimes however, the resulting GUI seems very diverse with less common elements, as shown in "Generation 4". The generation of these less common elements only happens if it is an often recurring structure within the dataset. So, for example, the first two rows of "Generation 4" occur 23 times as such in the training set. Which eventually leads to the model generating a BackgorundImage element at (2,2).

\paragraph{Generations with minimal Context -}

Another often recurring structure is the WebView GUI, as shown in Figure \ref{WebView-Example}. Our model frequently predicts this kind of GUI. As a general rule, we discovered that the more often a GUI structure is represented in the Dataset, the better it gets predicted with a low temperature. 

For example, we found at least 2k in the train set, as shown in Figure \ref{WebView-Example}. Therefore with a toolbar and only one WebView element in (2,3). Suppose the temperature is set to 0.2, then the model predicts this structure most of the time, either freely or with minimal context. Minimal context means we give the model only the first cube as a seed and only contain an image or an icon, as seen in Figure \ref{Minimal Seed}. This typically indicates the start of a toolbar. 

If we set the temperature to 0.5, the model predicts this structure less often. 
\begin{figure}[h!]
\centering
   \includegraphics[width=0.7\textwidth]{minimal seed.pdf}
  \caption{This abstraction was created by our model with minimal context. Even though we only gave one element as a context, it already predicted a WebView structure, as shown in Figure \ref{WebView-Example}. This is due to a dominance of this GUI layout in the dataset.}
  \label{Minimal Seed}
\end{figure}
If we give the model "$<$ 1", therefore \textit{"START TEXT"} as a seed, it will more likely predict a list-like GUI, since the text element in the first cube indicates the start of a list. It gets more likely if we use "$<$ 1\&2\&3", therefore \textit{"START Text\&Image\&Icon"} as a seed, due to the same reason. 

\subsubsection{GUI Generations with Context}

Our system shall not only generate GUIs randomly. The essence of our approach is that the model can grasp the idea of an initial sequence and auto-complete it to an entire GUI. Therefore it will set the grounds for a future system that can help GUI designers complete initial design ideas and propose different solutions to design challenges efficiently. 

Therefore, we test how well our system can auto-complete initial GUI-Language sequences. We do so by giving the model GUI-Language sequences from the test set and comparing the predictions to the original GUIs in the set. It is essential to only test the model with GUIs from the test set to guarantee a first exposure to the system with the GUI. 
We can already tell that our system is able to auto-complete the GUIs in Figure \ref{WebView-Example}, Figure \ref{List-Example} and Figure \ref{Grid-Example} well since it already generates similar GUIs without any context. 

For demonstration, we will first try to predict the named GUIs with our model. 

To ease the demonstration, we modified the pretty print of the system. The new presentation shows both the predicted as well as the original abstraction. The elements of the original abstraction are written in brackets on the right side of each cube. To be able to see differences in the generated abstraction directly, the generated elements are written on the left side of each cube. One can notice that not all cubes do contain generated elements. The reason is that the model does only start with predictions after the seed. 

As expected, one can see in Figure \ref{List-Test-Generation}, Figure \ref{Test-Generation}, and Figure \ref{Grid-Test-Generation}, the model works well in predicting patterns that are dominant in the dataset.  

In the next step, we will test if the model can auto-complete GUIs that are more diverse and less represented. Also, we want to evaluate the system on GUIs likely to be desired by potential users, such as login screens, menu panes, or others. 


\begin{figure}[h!]
\centering
   \includegraphics[width=\textwidth]{List-Gen_ex.pdf}
  \caption{Here we try to predict the GUI from Figure \ref{List-Example} by including its first row in the seed. As one can see, the prediction with a temperature of 0.2 only returns an empty GUI. This is a result of the dataset containing a significant amount of empty GUIs. Therefore if the seed does not create high enough conditional probabilities for other elements, only empty GUIs are generated. A way to overcome this issue is to set the temperature higher. With a temperature, the prediction is very well. The predicted GUI does show a similar list structure to the original GUI and only predicts some elements wrong. Namely, it does not list a WebView element in (2,4) or add an Icon in (1,3). The appendix shows an alternative generation with more given context in Figure \ref{List-Test-Generation-2}}
  \label{List-Test-Generation}
\end{figure}

\begin{figure}
\centering
    \begin{subfigure}{\textwidth}
        \includegraphics[width=1\linewidth]{Test-Generation.pdf}
        \caption{The system can very well predict the WebView GUI after only one given row. Even though the generation was done with a temperature of 0.2, the result does not look different from a temperature of 0.5.}
        \label{Test-Generation} 
    \end{subfigure}

    \begin{subfigure}{\textwidth}
       \includegraphics[width=1\linewidth]{Grid-Gen_ex.pdf}
       \caption{Here we try to predict the GUI from Figure \ref{Grid-Example} by including the first only giving the first row as a seed. As one can see, the model predicts the GUI wrong and generates a WebView GUI. However, if given slightly more context in Generation 2, the model perfectly imitates the original abstraction. The result was the same for Temperature 0.2 and 0.5.}
       \label{Grid-Test-Generation}
    \end{subfigure}

\end{figure}

\newpage
\subsubsection{Predicting more Diverse GUIs}
If we start predicting more diverse GUIs, which layouts are not as common in the dataset, we can notice the difficulties of our model. In Figure \ref{Login_Gen} as well as in Figure \ref{Menue_Pane-Gen} we try to auto-complete two layouts which should be more challenging to predict for our system, first a Login Screen and second Menu Panel. In both cases the model shall predict less common elements, to auto complete the abstraction with, in this case TextButtons and Inputs. We can control the diversity of the created elements to some extend by using a higher temperature during sampling. Doing so we are able to generate good results in one case and mediocre results in the other. However, if our model has to predict GUIs layouts that are rather individual, the predictions are very far of. An example can be seen in Figure \ref{RadioButton-gen-ex} and Figure \ref{Musik-player-gen}. Even though it is still partly able to capture the structure, it generally struggles to predict elements that which number is higher than 4 (see Table \ref{tabelle1}). 

\begin{figure}[h!]
\centering
 \includegraphics[width=1\linewidth]{Login_Gen_ex1.pdf}
        \caption{Here we try to predict a typical login screen consisting of Text, Input, and TextButton fields. As a seed, we provide the first row of the GUI-Abstraction. If we set the temperature to 0.2, the model predicts Text elements in (2,2), (2,3), and (2,4). It, therefore, does not catch the context and predicts the more common Text list instead of the desired Login Screen. 
        In contrast, if we set the temperature to 0.5, the system predicts the abstraction accurately. This is due to the system's likelihood of generating less common layouts if the temperature is high.}
        \label{Login_Gen} 
\end{figure}
\begin{figure}
\centering
    \begin{subfigure}{\textwidth}
       \includegraphics[width=1\linewidth]{Menue_Pane-Gen.pdf}
       \caption{With this trial, we tried to predict a standard menu panel with text buttons while giving the first row as a seed. With a temperature of 0.2, the model again predicts a Text list like in Figure \ref{Login_Gen}. If we set the temperature to 0.5, the model does predict the less common TextButton element but also captures the structure of the GUI less well.}
       \label{Menue_Pane-Gen}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=1\linewidth]{RadioButton-gen-ex.pdf}
       \caption{Judged by its layout and elements, the shown GUI is rarely represented in the training set, where no GUI-Abstraction has a RadioButton and Input element in the first row. Therefore, the prediction is empty if we try to predict the GUI by only its first row. If we add the second row to the seed, the model can predict parts of the GUI-Abstraction; however, the overall prediction is wildly inaccurate. This is a common problem if predicting less represented GUIs.}
       \label{RadioButton-gen-ex}
    \end{subfigure}
\end{figure}

\begin{figure}[h!]
\centering
 \includegraphics[width=1\linewidth]{Musik_Player_ex.pdf}
        \caption{As the GUI shown in Figure \ref{RadioButton-gen-ex}, this GUI includes less frequent GUI elements and has a less frequent layout. Even though our model does partly predict the common GUI elements Text, Image and Icon; it does not generate elements that are less common.}
        \label{Musik-player-gen} 
\end{figure}



\newpage

\newpage

\newpage

\subsection{Discussion}
\subsubsection{Internal Validity}
\subsubsection{External Validity}
We can notice three central issues.

First, our model does not predict GUIs themselves but GUI-Abstractions. If we abstract the GUI, a significant amount of information gets filtered. For example, by placing all elements in a Grid, we can categorize the GUI elements by location and filter out noise. However, the used grid is significantly coarse, leading to detailed layouts being less well represented. Not only do we lose information about the exact location of each element, but we also filter out the size of each element. 
Further, we filter out parent elements, which do contain little information by themselves but reflect valuable information about the layout of a GUI. To give an example, suppose we have a List-Element, a parent-element,  in a GUI. The List-Element does indicate that its child elements are part of a list and are aligned accordingly. Especially, Tool-Bar elements could be used to extract information about the GUIs layout by showing which element is part of the GUI itself or only of the toolbar. 

Limited to high frequency object in through data driven approach 

\subsubsection{Improvements / Validity}
\section{Lessons Learned}

\subsection{Basis for Further Work}
Our proposed tool can be used as a model that can help create new GUI design ideas like GuiGAN \cite{guigan}. If given an initial GUI layout idea it can create novel GUI designs, helping to extend ideas as stated in the Section \ref{INTRO}. 
Further, it can also function as a decoder in an encoder-decoder architecture. Therefore, future work could develop an complementary encoder and transistor to generate GUI designs based on text descriptions, as shown by Huang et al. (2021)\cite{GoogleHuang2021creating}.
\subsection{Limitations}
-sequential data, not possible to give context by starting in the middle of the GUI
-"like most data-driven approaches, generated layouts are dominated by high frequency objects or shapes in the dataset. We can control the diversity to some extent using improved sam- pling techniques, however, generating diverse layouts that not only learn from data, but also from human priors or pre- defined rules is an important direction of research which we will continue to explore"\cite{layouttransformer}
-RICO heuristicen verbessern 
\section{Acknowledgments}
- Implementation is given on top of KK
\section{Further Information}\label{FurtherInfo}
- Git
- Kristians tool
\newpage
\bibliographystyle{spmpsci}
\bibliography{refs}
\section{Appendix}
\begin{figure}[h!]
\subsection{Additional GUI Generations}
\centering
   \includegraphics[width=\textwidth]{Lsut-gen-ex2.pdf}
  \caption{Here we try to predict the GUI from Figure \ref{List-Example} by including the first two rows in the seed. As one can see, both temperatures return an similar outcome, with the Abstraction roughly fitting the list structure. Nevertheless, some elements are left out during predictions while some are added. }
  \label{List-Test-Generation-2}
\end{figure}
\subsection{Hyper-parameter Tuning}
Here one can find the models console output for the described hyperparameter tuning process.

\end{document}
